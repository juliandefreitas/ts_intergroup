length(ts_forced[ts_forced == 1 & cond_num == 1])/length(ts_forced[cond_num == 1]) #proportion of improvement condition attributed to ts
length(ts_forced[ts_forced == 1 & cond_num == 2])/length(ts_forced[cond_num == 2]) #proportion of deterioration condition attributed to ts
##========================= true self scaled rating =====================================
ts_scale_aov <- aov(ts_scale ~ cond_num*vignette_num*eth_num*order_num, data = data)
summary(ts_scale_aov)
etaSquared(ts_scale_aov)
#adjusted alphas
cond_aa_ts <- 0.05/15
vignette_aa_ts <- 0.05/14
eth_aa_ts <- 0.05/13
n_det <- table(cond)[1]
n_imp <- table(cond)[2]
ts_scale_mean <- tapply(ts_scale,cond,mean); ts_scale_mean
ts_scale_sd <- tapply(ts_scale,cond,sd); ts_scale_sd
ts_scaled_t <- t.test(ts_scale ~ cond, var.equal=TRUE, paired=FALSE); ts_scaled_t
tes(as.numeric(ts_scaled_t[1]), n_det, n_imp) #cohen's d
##====================== plot true self scaled rating ========================
alpha <- 0.05
condNames <- c("Improvement","Deterioration")
ethNames <- c("White", "Immigrant")
## condition effect by vignette
ts_vignettes <- matrix(NA,12,6)
colnames(ts_vignettes) <- c('cond','vignette','mean','sd','n','sem')
count <- 1
for(i in 1:2) {
for(k in 1:6) {
ts_vignettes[count,] <- c(i,k,mean(ts_scale[cond_num == i & vignette_num == k]),sd(ts_scale[cond_num == i & vignette_num == k]),length(ts_scale[cond_num == i & vignette_num == k]),0)
ts_vignettes[count,6] <- ts_vignettes[count,4]/sqrt(ts_vignettes[count,5]) #calculate standard error from mean
count = count+1;
}
}
ts_vignettes.summary <- as.data.frame(ts_vignettes, stringsAsFactors=F)
## effect of condition by vignette (fig. S1)
title <- c("Effct of Condition by Vignette")
p1<-ggplot(ts_vignettes.summary,aes(x=factor(cond),y=mean,fill=factor(vignette))) +
stat_summary(fun.y=mean,position=position_dodge(),geom="bar")+theme_bw()+coord_cartesian(ylim=c(1,9))+scale_y_discrete(limits=c("1","2","3","4","5","6","7","8","9"))
p1 <- p1+theme(text = element_text(size=16),panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
geom_errorbar(aes(ymax=mean+sem, ymin=mean-sem), position="dodge")+ggtitle(title)+scale_x_discrete(breaks = 1:length(condNames), labels=condNames)+
xlab("Condition")+ylab("Mean True Self Attributions")+scale_fill_discrete(name="Vignette",labels=c("Caring/deadbeat father","Against/support terrorism","Ethical/unethical businessman","Teetotaler/alcoholic","Respect/mistreat minorities","Honest/corrupt officer"))+theme(legend.key=element_blank())
p1
## effect of condition by vignette (fig. S1)
title <- c("Effect of Condition by Vignette")
p1<-ggplot(ts_vignettes.summary,aes(x=factor(cond),y=mean,fill=factor(vignette))) +
stat_summary(fun.y=mean,position=position_dodge(),geom="bar")+theme_bw()+coord_cartesian(ylim=c(1,9))+scale_y_discrete(limits=c("1","2","3","4","5","6","7","8","9"))
p1 <- p1+theme(text = element_text(size=16),panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
geom_errorbar(aes(ymax=mean+sem, ymin=mean-sem), position="dodge")+ggtitle(title)+scale_x_discrete(breaks = 1:length(condNames), labels=condNames)+
xlab("Condition")+ylab("Mean True Self Attributions")+scale_fill_discrete(name="Vignette",labels=c("Caring/deadbeat father","Against/support terrorism","Ethical/unethical businessman","Teetotaler/alcoholic","Respect/mistreat minorities","Honest/corrupt officer"))+theme(legend.key=element_blank())
p1
rm(list = ls())
## necessary packages
if (!require(ggplot2)) {install.packages("ggplot2"); require(ggplot2)}
if (!require(plyr)) {install.packages("plyr"); require(plyr)}
if (!require(lsr)) {install.packages("lsr"); require(lsr)}
if (!require(psy)) {install.packages("psy"); require(psy)}
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
if (!require(lmtest)) {install.packages("lmtest"); require(lmtest)}
if (!require(bbmle)) {install.packages("bbmle"); require(bbmle)}
if (!require(conting)) {install.packages("conting"); require(conting)}
## get data
dir <- setwd("/Users/Julian/Dropbox (Personal)/Research/socialPsych/intergroupTS/intergroupTS_onlineMaterials/e1_trueSelfBias/data_and_analyses")
data <- read.csv("exp1_trueSelfBias.csv")
dim(data) # 1020
## get rid of unnecessary rows
data <- data[,c(10,11,344:362)]
data <- subset(data,(data$ethn_ss == 6 & data$exclude == 0))
dim(data) # 613
## assign variable names
threat <- as.numeric(data$threat.28)
attitude <- as.numeric(data$attitude.34)
identification <-as.numeric(data$identification)
ts_scale <- as.numeric(data$ts_scaled)
ts_forced <- as.factor(data$ts_forced_recode)
cond <- as.factor(data$cond_name)
cond_num <- as.factor(data$condition)
vignette <- as.factor(data$vignette_name)
vignette_num <- as.factor(data$vignette)
eth <- as.factor(data$eth_cond_name)
eth_num <- as.factor(data$eth_cond)
gender <- as.factor(data$Gender)
age <- as.numeric(levels(data$Age))[data$Age]
table(eth,cond)
data$ts_forced_recode
data$ts_forced_recode <- data$ts_forced_recode + 1
dataExport <- data[,c(7,9,11,19,20)]
dataExport
head(dataExport)
dim(dataExport)
data$ts_forced_recode <- data$ts_forced_recode + 1
dataExport <- data[,c(7,9,11,19,20)]
dataExport_final <- as.data.frame(dataExport)
write.csv(dataExport_final,"e1_tsIntergroup.csv")
mean(age,na.rm = TRUE) #35.95
table(gender)[2]/sum(table(gender)) #53.51%
english_syllables <- c(1,2,2,2,1,2)
arab_syllables <- c(3,2,2,2,2,2)
t.test(english_syllables,arab_syllables) #p = .094
english_characters <- c(2,6,6,7,4,4)
arab_characters <- c(7,6,5,5,6,5)
t.test(english_characters, arab_characters) #p = .344
english_vowels <- c(1,3,2,2,1,2)
arab_vowels <- c(3,2,2,2,2,2)
t.test(english_vowels, arab_vowels) #p=.369
english_consonants <- c(1,3,4,5,4,2)
arab_consonants <- c(4,4,3,3,4,3)
t.test(english_consonants,arab_consonants) #p=.621
ident_mat_white <- array(0,dim=c(219,3))
ident_mat_white[,1] <- data$value.36[data$eth_cond_name == "white"]
ident_mat_white[,2] <- data$like.36[data$eth_cond_name == "white"]
ident_mat_white[,3] <- data$connected.36[data$eth_cond_name == "white"]
cronbach(ident_mat_white) #0.86
ident_mat_native <- array(0,dim=c(194,3))
ident_mat_native[,1] <- data$value.36[data$eth_cond_name == "syrian"]
ident_mat_native[,2] <- data$like.36[data$eth_cond_name == "syrian"]
ident_mat_native[,3] <- data$connected.36[data$eth_cond_name == "syrian"]
cronbach(ident_mat_native) #0.88
ident_mat_immigrant <- array(0,dim=c(200,3))
ident_mat_immigrant[,1] <- data$value.36[data$eth_cond_name == "immigrant"]
ident_mat_immigrant[,2] <- data$like.36[data$eth_cond_name == "immigrant"]
ident_mat_immigrant[,3] <- data$connected.36[data$eth_cond_name == "immigrant"]
cronbach(ident_mat_immigrant) #0.90
## 6 July 2016
## Author: Julian De Freitas, Harvard University
## clear workpace
rm(list = ls())
## necessary packages
if (!require(ggplot2)) {install.packages("ggplot2"); require(ggplot2)}
if (!require(plyr)) {install.packages("plyr"); require(plyr)}
if (!require(lsr)) {install.packages("lsr"); require(lsr)}
if (!require(psy)) {install.packages("psy"); require(psy)}
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
if (!require(lmtest)) {install.packages("lmtest"); require(lmtest)}
if (!require(bbmle)) {install.packages("bbmle"); require(bbmle)}
if (!require(conting)) {install.packages("conting"); require(conting)}
## get data
dir <- setwd("/Users/Julian/Dropbox (Personal)/Research/socialPsych/intergroupTS/intergroupTS_onlineMaterials/e1_trueSelfBias/data_and_analyses")
data <- read.csv("exp1_trueSelfBias.csv")
dim(data) # 1020
## get rid of unnecessary rows
data <- data[,c(10,11,344:362)]
## exclusions: only incl. white subjects who answered attention and comprehension questions correctly
data <- subset(data,(data$ethn_ss == 6 & data$exclude == 0))
dim(data) # 613
## assign variable names
threat <- as.numeric(data$threat.28)
attitude <- as.numeric(data$attitude.34)
identification <-as.numeric(data$identification)
ts_scale <- as.numeric(data$ts_scaled)
ts_forced <- as.factor(data$ts_forced_recode)
cond <- as.factor(data$cond_name)
cond_num <- as.factor(data$condition)
vignette <- as.factor(data$vignette_name)
vignette_num <- as.factor(data$vignette)
eth <- as.factor(data$eth_cond_name)
eth_num <- as.factor(data$eth_cond)
gender <- as.factor(data$Gender)
age <- as.numeric(levels(data$Age))[data$Age]
table(eth,cond)
## Mean age and gender
mean(age,na.rm = TRUE) #35.95
table(gender)[2]/sum(table(gender)) #53.51%
###-- Aside: Export data for bayes analysis to be conducted in separate R script and in JASP
data$ts_forced_recode <- data$ts_forced_recode + 1
dataExport <- data[,c(7,9,11,19,20)]
dataExport_final <- as.data.frame(dataExport)
write.csv(dataExport_final,"e1_tsIntergroup.csv")
#syallables
english_syllables <- c(1,2,2,2,1,2)
arab_syllables <- c(3,2,2,2,2,2)
t.test(english_syllables,arab_syllables) #p = .094
#characters
english_characters <- c(2,6,6,7,4,4)
arab_characters <- c(7,6,5,5,6,5)
t.test(english_characters, arab_characters) #p = .344
#vowels
english_vowels <- c(1,3,2,2,1,2)
arab_vowels <- c(3,2,2,2,2,2)
t.test(english_vowels, arab_vowels) #p=.369
#consonants
english_consonants <- c(1,3,4,5,4,2)
arab_consonants <- c(4,4,3,3,4,3)
t.test(english_consonants,arab_consonants) #p=.621
ethCondNames <- c("White","Immigrant","Syrian")
n_white <- table(eth)[3]; n_white
n_native <- table(eth)[2]; n_native
n_immigrant <- table(eth)[1]; n_immigrant
threat_aov <- aov(threat ~ eth_num, data = data)
summary(threat_aov)
etaSquared(threat_aov)
if (!require(ggplot2)) {install.packages("ggplot2"); require(ggplot2)}
if (!require(plyr)) {install.packages("plyr"); require(plyr)}
if (!require(lsr)) {install.packages("lsr"); require(lsr)}
if (!require(psy)) {install.packages("psy"); require(psy)}
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
if (!require(lmtest)) {install.packages("lmtest"); require(lmtest)}
if (!require(bbmle)) {install.packages("bbmle"); require(bbmle)}
if (!require(scales)) {install.packages("scales"); require(scales)}
english_characters <- c(2,6,6,7,4,4)
arab_characters <- c(7,6,5,5,6,5)
character_test <- t.test(english_characters, arab_characters) #p = .344
tes(as.numeric(character_test[1]), 6, 6) #cohen's d
character_test <- t.test(english_characters, arab_characters) #p = .344
character_test
test(as.numeric(-1.0164),6,6)
tes(as.numeric(-1.0164),6,6)
english_syllables <- c(1,2,2,2,1,2)
arab_syllables <- c(3,2,2,2,2,2)
syllable_test <- t.test(english_syllables,arab_syllables) #p = .094
test(as.numeric(syllable_test), 6, 6,)
tes(as.numeric(syllable_test), 6, 6,)
tes(as.numeric(syllable_test), 6, 6)
tes(as.numeric(syllable_test[1]), 6, 6)
english_syllables <- c(1,2,2,2,1,2)
arab_syllables <- c(3,2,2,2,2,2)
syllable_test <- t.test(english_syllables,arab_syllables) #p = .094
tes(as.numeric(syllable_test[1]), 6, 6) #cohen's d =
english_characters <- c(2,6,6,7,4,4)
arab_characters <- c(7,6,5,5,6,5)
character_test <- t.test(english_characters, arab_characters) #p = .344
tes(as.numeric(character_test[1]), 6, 6) #cohen's d =
english_vowels <- c(1,3,2,2,1,2)
arab_vowels <- c(3,2,2,2,2,2)
vowel_test <- t.test(english_vowels, arab_vowels) #p=.369
test(as.numeric(vowel_test),6, 6) #cohen's d =
tes(as.numeric(vowel_test),6, 6) #cohen's d =
tes(as.numeric(vowel_test[1]),6, 6) #cohen's d =
english_consonants <- c(1,3,4,5,4,2)
arab_consonants <- c(4,4,3,3,4,3)
consonant_test <- t.test(english_consonants,arab_consonants) #p=.621
test(as.numeric(consonant_test[1]), 6, 6) #cohen's d =
tes(as.numeric(consonant_test[1]), 6, 6) #cohen's d =
consonant_test <- t.test(english_consonants,arab_consonants) #p=.621
consonant_test[1]
tes(as.numeric(syllable_test[1]), 6, 6) #cohen's d = 1.07
english_syllables <- c(1,2,2,2,1,2)
arab_syllables <- c(3,2,2,2,2,2)
syllable_test <- t.test(english_syllables,arab_syllables) #p = .094
tes(as.numeric(syllable_test[1]), 6, 6) #cohen's d = 1.07
syllable_test[1]
## 6 July 2016
## Author: Julian De Freitas, Harvard University
## clear workpace
rm(list = ls())
## necessary packages
if (!require(ggplot2)) {install.packages("ggplot2"); require(ggplot2)}
if (!require(plyr)) {install.packages("plyr"); require(plyr)}
if (!require(lsr)) {install.packages("lsr"); require(lsr)}
if (!require(psy)) {install.packages("psy"); require(psy)}
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
if (!require(lmtest)) {install.packages("lmtest"); require(lmtest)}
if (!require(broom)) {install.packages("broom"); require(broom)}
if (!require(bbmle)) {install.packages("bbmle"); require(bbmle)}
if (!require(lsmeans)) {install.packages("lsmean"); require(lsmeans)}
if (!require(scales)) {install.packages("scales"); require(scales)}
## get data
dir <- setwd("/Users/Julian/Dropbox (Personal)/Research/socialPsych/intergroupTS/intergroupTS_onlineMaterials/e2_eliminate_judgments/data_and_analyses")
data <- read.csv("exp2_eliminate_judgments.csv")
dim(data) #1327
## remove unnecessary columns
data <- data[,c(9,10,403:422)]
## exclusions: only incl. white subjects who answered attention and comprehension checks correctly
data <- subset(data,(data$eth_ss == 6 & data$exclude == 0))
dim(data) #759
## assign variable names
threat <- as.numeric(data$THREAT.43)
attitude <- as.numeric(data$ATTITUDE.43)
identification <-as.numeric(data$IDENTIFICATION)
ts_scale <- as.numeric(data$TS_SCALE)
ts_forced <- as.factor(data$TS_FORCED_RECODE)
cond <- as.factor(data$COND_NAME)
cond_num <- as.factor(data$COND)
vignette <- as.factor(data$VIGNETTE_NAME)
vignette_num <- as.factor(data$VIGNETTE)
eth <- as.factor(data$ETH_COND_NAME)
eth_num <- as.factor(data$ETH_COND)
order_num <- as.factor(data$ORDER)
order <- vector(mode="character", length=dim(data)[1])
for (i in 1:dim(data)[1]) {
if(order_num[i] == 1) {
order[i] = 'intergroup first'
}
else if(order_num[i] == 2) {
order[i] = 'ts first'
}
}
gender <- as.factor(data$Gender)
age <- as.numeric(data$Age)
n_immigrant <- table(eth)[1]
n_white <- table(eth)[2]
## mean age and gender
mean(age,na.rm = TRUE) #37.14
table(gender)[2]/sum(table(gender)) #56.55%
##===== Aside: Export data for bayes analysis to be conducted in separate script ======
#dataExport <- data[,c(8,10,12,13,21,22)]
#dataExport <- as.data.frame(dataExport)
#write.csv(dataExport,"e2_tsIntergroup.csv")
##========================= true self forced choice =====================================
length(ts_forced[ts_forced == 1 & cond_num == 1])/length(ts_forced[cond_num == 1]) #proportion of improvement condition attributed to ts
length(ts_forced[ts_forced == 1 & cond_num == 2])/length(ts_forced[cond_num == 2]) #proportion of deterioration condition attributed to ts
ts_forced_glm_1 <- glm(ts_forced ~ cond_num, family = binomial)
summary(ts_forced_glm_1)
#see separate script for bayesian logistic regression in R
###===================== plot - true self forced choice (supp fig. 2) ===================
ethCondNames <- c("White American","Arab Immigrant")
accuracyTable <- matrix(NA,2,2)
rownames(accuracyTable) <- c('White American','Arab Immigrant')
colnames(accuracyTable) <- c('Improvement','Deterioration')
accuracyTable[1,1] <- sum(as.numeric(ts_forced[eth_num == 1 & cond_num == 1])-1)/length(ts_forced[eth_num == 1 & cond_num == 1])
accuracyTable[1,2] <- sum(as.numeric(ts_forced[eth_num == 1 & cond_num == 2])-1)/length(ts_forced[eth_num == 1 & cond_num == 2])
accuracyTable[2,1] <- sum(as.numeric(ts_forced[eth_num == 2 & cond_num == 1])-1)/length(ts_forced[eth_num == 2 & cond_num == 1])
accuracyTable[2,2] <- sum(as.numeric(ts_forced[eth_num == 2 & cond_num == 2])-1)/length(ts_forced[eth_num == 2 & cond_num == 2])
colors <- hue_pal()(3)
p = barplot(accuracyTable, beside=TRUE,ylim=c(0,1.0), col=colors[1:2],cex.names=1.5,cex.main=1.2,cex.main=1.7,
cex.lab=1.5,legend.text=FALSE,ylab="Proportion of Participants Attributing Change to True Self",main="True Self Forced Choice Items", family = "Helvetica")
legend("top", legend = ethCondNames, fill = colors[1:2], bty="n",cex = 1.3)
##========================= true self scaled rating =====================================
ts_scale_aov <- aov(ts_scale ~ cond_num*vignette_num*eth_num*order_num, data = data)
summary(ts_scale_aov)
etaSquared(ts_scale_aov)
## 6 July 2016
## Author: Julian De Freitas, Harvard University
## clear workpace
rm(list = ls())
## necessary packages
if (!require(ggplot2)) {install.packages("ggplot2"); require(ggplot2)}
if (!require(plyr)) {install.packages("plyr"); require(plyr)}
if (!require(lsr)) {install.packages("lsr"); require(lsr)}
if (!require(psy)) {install.packages("psy"); require(psy)}
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
if (!require(lmtest)) {install.packages("lmtest"); require(lmtest)}
if (!require(broom)) {install.packages("broom"); require(broom)}
if (!require(bbmle)) {install.packages("bbmle"); require(bbmle)}
if (!require(lsmeans)) {install.packages("lsmean"); require(lsmeans)}
if (!require(scales)) {install.packages("scales"); require(scales)}
## get data
dir <- setwd("/Users/Julian/Dropbox (Personal)/Research/socialPsych/intergroupTS/intergroupTS_onlineMaterials/e2_eliminate_judgments/data_and_analyses")
data <- read.csv("exp2_eliminate_judgments.csv")
dim(data) #1327
## remove unnecessary columns
data <- data[,c(9,10,403:422)]
## exclusions: only incl. white subjects who answered attention and comprehension checks correctly
data <- subset(data,(data$eth_ss == 6 & data$exclude == 0))
dim(data) #759
## assign variable names
threat <- as.numeric(data$THREAT.43)
attitude <- as.numeric(data$ATTITUDE.43)
identification <-as.numeric(data$IDENTIFICATION)
ts_scale <- as.numeric(data$TS_SCALE)
ts_forced <- as.factor(data$TS_FORCED_RECODE)
cond <- as.factor(data$COND_NAME)
cond_num <- as.factor(data$COND)
vignette <- as.factor(data$VIGNETTE_NAME)
vignette_num <- as.factor(data$VIGNETTE)
eth <- as.factor(data$ETH_COND_NAME)
eth_num <- as.factor(data$ETH_COND)
order_num <- as.factor(data$ORDER)
order <- vector(mode="character", length=dim(data)[1])
for (i in 1:dim(data)[1]) {
if(order_num[i] == 1) {
order[i] = 'intergroup first'
}
else if(order_num[i] == 2) {
order[i] = 'ts first'
}
}
gender <- as.factor(data$Gender)
age <- as.numeric(data$Age)
n_immigrant <- table(eth)[1]
n_white <- table(eth)[2]
## mean age and gender
mean(age,na.rm = TRUE) #37.14
table(gender)[2]/sum(table(gender)) #56.55%
##===== Aside: Export data for bayes analysis to be conducted in separate script ======
#dataExport <- data[,c(8,10,12,13,21,22)]
#dataExport <- as.data.frame(dataExport)
#write.csv(dataExport,"e2_tsIntergroup.csv")
##========================= true self forced choice =====================================
length(ts_forced[ts_forced == 1 & cond_num == 1])/length(ts_forced[cond_num == 1]) #proportion of improvement condition attributed to ts
length(ts_forced[ts_forced == 1 & cond_num == 2])/length(ts_forced[cond_num == 2]) #proportion of deterioration condition attributed to ts
ts_forced_glm_1 <- glm(ts_forced ~ cond_num, family = binomial)
summary(ts_forced_glm_1)
#see separate script for bayesian logistic regression in R
###===================== plot - true self forced choice (supp fig. 2) ===================
ethCondNames <- c("White American","Arab Immigrant")
accuracyTable <- matrix(NA,2,2)
rownames(accuracyTable) <- c('White American','Arab Immigrant')
colnames(accuracyTable) <- c('Improvement','Deterioration')
accuracyTable[1,1] <- sum(as.numeric(ts_forced[eth_num == 1 & cond_num == 1])-1)/length(ts_forced[eth_num == 1 & cond_num == 1])
accuracyTable[1,2] <- sum(as.numeric(ts_forced[eth_num == 1 & cond_num == 2])-1)/length(ts_forced[eth_num == 1 & cond_num == 2])
accuracyTable[2,1] <- sum(as.numeric(ts_forced[eth_num == 2 & cond_num == 1])-1)/length(ts_forced[eth_num == 2 & cond_num == 1])
accuracyTable[2,2] <- sum(as.numeric(ts_forced[eth_num == 2 & cond_num == 2])-1)/length(ts_forced[eth_num == 2 & cond_num == 2])
colors <- hue_pal()(3)
p = barplot(accuracyTable, beside=TRUE,ylim=c(0,1.0), col=colors[1:2],cex.names=1.5,cex.main=1.2,cex.main=1.7,
cex.lab=1.5,legend.text=FALSE,ylab="Proportion of Participants Attributing Change to True Self",main="True Self Forced Choice Items", family = "Helvetica")
legend("top", legend = ethCondNames, fill = colors[1:2], bty="n",cex = 1.3)
##========================= true self scaled rating =====================================
ts_scale_aov <- aov(ts_scale ~ cond_num*vignette_num*eth_num*order_num, data = data)
summary(ts_scale_aov)
etaSquared(ts_scale_aov)
#adjusted alphas
cond_aa_ts <- 0.05/15; cond_aa_ts
vignette_aa_ts <- 0.05/14; vignette_aa_ts
eth_aa_ts <- 0.05/13; eth_aa_ts
n_det <- table(cond)[1]
n_imp <- table(cond)[2]
#see separate JASP file for bayesian anova
ts_scale_mean <- tapply(ts_scale,cond,mean); ts_scale_mean
ts_scale_sd <- tapply(ts_scale,cond,sd); ts_scale_sd
ts_scaled_t <- t.test(ts_scale ~ cond, var.equal=TRUE, paired=FALSE); ts_scaled_t
tes(as.numeric(ts_scaled_t[1]), n_det, n_imp) #cohen's d
##====================== plot true self scaled rating ========================
alpha <- 0.05
condNames <- c("Improvement","Deterioration")
## condition effect by vignette
ts_vignettes <- matrix(NA,12,6)
colnames(ts_vignettes) <- c('cond','vignette','mean','sd','n','sem')
count <- 1
for(i in 1:2) {
for(k in 1:6) {
ts_vignettes[count,] <- c(i,k,mean(ts_scale[cond_num == i & vignette_num == k]),sd(ts_scale[cond_num == i & vignette_num == k]),length(ts_scale[cond_num == i & vignette_num == k]),0)
ts_vignettes[count,6] <- ts_vignettes[count,4]/sqrt(ts_vignettes[count,5]) #calculate standard error from mean
count = count+1;
}
}
ts_vignettes.summary <- as.data.frame(ts_vignettes, stringsAsFactors=F)
## effect of condition by vignette (supp fig. S3)
title <- c("Effect of Condition by Vignette")
p1<-ggplot(ts_vignettes.summary,aes(x=factor(cond),y=mean,fill=factor(vignette))) +
stat_summary(fun.y=mean,position=position_dodge(),geom="bar")+theme_bw()+coord_cartesian(ylim=c(1,9))+scale_y_discrete(limits=c("1","2","3","4","5","6","7","8","9"))
p1 <- p1+theme(text = element_text(size=18, family="Helvetica"),panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
geom_errorbar(aes(ymax=mean+sem, ymin=mean-sem), position="dodge")+ggtitle(title)+scale_x_discrete(breaks = 1:length(condNames), labels=condNames)+
xlab("Condition")+ylab("Mean True Self Attributions")+scale_fill_discrete(name="Vignette",labels=c("Caring/deadbeat father","Against/support terrorism","Ethical/unethical businessman","Teetotaler/alcoholic","Respect/mistreat minorities","Honest/corrupt officer"))+theme(legend.key=element_blank())+
theme(plot.title = element_text(hjust = 0.5))
p1
##=========================== threat ratings =================================
n_immigrant_1 = table(eth,order_num)[1,1]
n_white_1 = table(eth,order_num)[2,1]
n_immigrant_2 = table(eth,order_num)[1,2]
n_white_2 = table(eth,order_num)[2,2]
#How much do you agree: People from white/Arab backgrounds like Al/Alhadin threaten the
#american way of life
threat_aov <- aov(threat ~ vignette_num*eth*cond_num*order)
summary(threat_aov)
etaSquared(threat_aov)
## 6 July 2016
## Author: Julian De Freitas, Harvard University
## clear workpace
rm(list = ls())
## necessary packages
if (!require(ggplot2)) {install.packages("ggplot2"); require(ggplot2)}
if (!require(plyr)) {install.packages("plyr"); require(plyr)}
if (!require(lsr)) {install.packages("lsr"); require(lsr)}
if (!require(psy)) {install.packages("psy"); require(psy)}
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
if (!require(lmtest)) {install.packages("lmtest"); require(lmtest)}
if (!require(bbmle)) {install.packages("bbmle"); require(bbmle)}
if (!require(scales)) {install.packages("scales"); require(scales)}
## get data
dir <- setwd("/Users/Julian/Dropbox (Personal)/Research/socialPsych/intergroupTS/intergroupTS_onlineMaterials/e1_trueSelfBias/data_and_analyses")
data <- read.csv("exp1_trueSelfBias.csv")
dim(data) # 1020
## get rid of unnecessary rows
data <- data[,c(10,11,344:362)]
## exclusions: only incl. white subjects who answered attention and comprehension questions correctly
data <- subset(data,(data$ethn_ss == 6 & data$exclude == 0))
dim(data) # 613
## assign variable names
threat <- as.numeric(data$threat.28)
attitude <- as.numeric(data$attitude.34)
identification <-as.numeric(data$identification)
ts_scale <- as.numeric(data$ts_scaled)
ts_forced <- as.factor(data$ts_forced_recode)
cond <- as.factor(data$cond_name)
cond_num <- as.factor(data$condition)
vignette <- as.factor(data$vignette_name)
vignette_num <- as.factor(data$vignette)
eth <- as.factor(data$eth_cond_name)
eth_num <- as.factor(data$eth_cond)
gender <- as.factor(data$Gender)
age <- as.numeric(levels(data$Age))[data$Age]
table(eth,cond)
## Mean age and gender
mean(age,na.rm = TRUE) #35.95
table(gender)[2]/sum(table(gender)) #53.51%
##===== Aside: Export data for bayes analysis to be conducted in separate script ======
#dataExport <- data[,c(7,9,11,19,20)]
#dataExport <- as.data.frame(dataExport)
#write.csv(dataExport,"e1_tsIntergroup.csv")
##========================= comparison of english vs. arab names =======================
#Al v Alhadin
#Daniel v Danyal
#Harris v Haris
#Jeffrey v Jafri
#Chad v Chahid
#Adam v Adnan
#syallables
english_syllables <- c(1,2,2,2,1,2)
mean_english_syllables = mean(english_syllables)
mean_english_syllables = mean(english_syllables)
std_english_syllables = std(english_syllables)
std_english_syllables = stdev(english_syllables)
std_english_syllables = std(english_syllables)
std_english_syllables = sd(english_syllables)
std_arab_syllables = sd(arab_syllables)
std_arab_syllables = sd(arab_syllables)
std_arab_syllables = sd(arab_syllables)
arab_syllables <- c(3,2,2,2,2,2)
mean_arab_syllables = mean(arab_syllables)
std_arab_syllables = sd(arab_syllables)
mean_english_syllables = mean(english_syllables); mean_english_syllables
std_english_syllables = sd(english_syllables); std_english_syllables
mean_arab_syllables = mean(arab_syllables); mean_arab_syllables
std_arab_syllables = sd(arab_syllables); std_arab_syllables
