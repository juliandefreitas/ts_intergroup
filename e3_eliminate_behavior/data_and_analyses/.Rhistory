p1 <- p1+theme(text = element_text(size=18, family="Helvetica"),panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
geom_errorbar(aes(ymax=mean+sem, ymin=mean-sem), position="dodge")+ggtitle(title)+scale_x_discrete(breaks = 1:length(condNames), labels=condNames)+
xlab("Condition")+ylab("Mean True Self Attributions")+scale_fill_discrete(name="Vignette",labels=c("Caring/deadbeat father","Against/support terrorism","Ethical/unethical businessman","Teetotaler/alcoholic","Respect/mistreat minorities","Honest/corrupt officer"))+theme(legend.key=element_blank())+
theme(plot.title = element_text(hjust = 0.5))
p1
##=========================== threat ratings =================================
n_immigrant_1 = table(eth,order_num)[1,1]
n_white_1 = table(eth,order_num)[2,1]
n_immigrant_2 = table(eth,order_num)[1,2]
n_white_2 = table(eth,order_num)[2,2]
#How much do you agree: People from white/Arab backgrounds like Al/Alhadin threaten the
#american way of life
threat_aov <- aov(threat ~ vignette_num*eth*cond_num*order)
summary(threat_aov)
etaSquared(threat_aov)
#adjusted alphas
eth_by_order_aa_threat <- 0.05/15; eth_by_order_aa_threat
order_aa_threat <- 0.05/14; order_aa_threat
eth_aa_threat <- 0.05/13; eth_aa_threat
vignette_aa_threat <- 0.05/12; vignette_aa_threat
vignette_order_aa_threat <- 0.05/11; vignette_order_aa_threat
#lsmeans
means <- lsmeans(threat_aov, specs = ~ eth* order); means
tests <- contrast(means, method="pairwise"); tests
confint(tests)
tes(5.186, n_white_1, n_immigrant_1) #cohen's d
tes(-1.086, n_white_2, n_immigrant_2) #cohen's d
##=========================== plot threat ratings (fig. 2) ===================================
alpha <- 0.05
ethCondNames <- c("White American","Arab Immigrant")
out_mat_names <- c('threat','attitude','identification')
#OVERALL
outgroup_mat_overall <- array(0,dim=c(4,7,3)) #averaging across vignettes
colnames(outgroup_mat_overall) <- c('eth','order','mean','sd','n','sem','me')
count <- 1
for (h in 1:3) {
count = 1
for(k in 1:2) {
for(l in 1:2) {
outgroup_mat_overall[count,,h] <- c(k,l,mean(get(out_mat_names[h])[eth_num == k & order_num == l]),sd(get(out_mat_names[h])[eth_num == k & order_num == l]),length(get(out_mat_names[h])[eth_num == k & order_num == l]),0,0)
outgroup_mat_overall[count,6,h] <- outgroup_mat_overall[count,4,h]/sqrt(outgroup_mat_overall[count,5,h]) #calculate standard error from mean
outgroup_mat_overall[count,7,h] <- qt(1-alpha/2, df=outgroup_mat_overall[count,5,h]*outgroup_mat_overall[count,6,h]) #calculate margin of error
count = count+1
}
}
}
outgroup_mat.overall <- as.data.frame(outgroup_mat_overall, stringsAsFactors=F)
colnames(outgroup_mat.overall) <- c('eth','order','mean','sd','n','sem','me','eth','order','mean','sd','n','sem','me','eth','order','mean','sd','n','sem','me')
## overall means (fig. 2)
p2<-ggplot(outgroup_mat.overall[,1:7],aes(x=factor(order),y=mean,fill=factor(eth)),color=factor(eth)) +
stat_summary(fun.y=mean,position=position_dodge(),geom="bar")+theme_bw()+coord_cartesian(ylim=c(1,9))+scale_y_discrete(limits=c("1","2","3","4","5","6","7","8","9"))
p2<-p2+theme(text = element_text(size=16),panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
geom_errorbar(aes(ymax=mean+sem, ymin=mean-sem), position="dodge")+scale_x_discrete(breaks = 1:length(ethCondNames), labels=c("Threat First", "TS First"))+
xlab("Order of Measures")+ylab("Mean Threat Ratings")+scale_fill_discrete(name="Ethnicity",labels=c("White","Immigrant"))+theme(legend.key=element_blank())
p2
##=========================== attitude ratings =================================
#How do you feel about people from white/Arab backgrounds like Al/Alhadin? (1=cold,9=warm)
attitude_aov <- aov(attitude ~ vignette_num*eth*cond_num*order)
summary(attitude_aov)
etaSquared(attitude_aov)
#adjusted alphas
eth_order_aa_attitude <- 0.05/15; eth_order_aa_attitude
vignette_eth_cond_aa_attitude <- 0.05/14; vignette_eth_cond_aa_attitude
vignette_aa_attitude <- 0.05/13; vignette_aa_attitude
#lsmeans
means <- lsmeans(attitude_aov, specs = ~ eth* order); means
tests <- contrast(means, method="pairwise"); tests
confint(tests)
tes(3.124, n_white_1, n_immigrant_1) #cohen's d
tes(0.930, n_white_2, n_immigrant_2) #cohen's d
## plot attitude ratings (fig. 2)
p3<-ggplot(outgroup_mat.overall[,8:14],aes(x=factor(order),y=mean,fill=factor(eth)),color=factor(eth)) +
stat_summary(fun.y=mean,position=position_dodge(),geom="bar")+theme_bw()+coord_cartesian(ylim=c(1,9))+scale_y_discrete(limits=c("1","2","3","4","5","6","7","8","9"))
p3<- p3+theme(text = element_text(size=16),panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
geom_errorbar(aes(ymax=mean+sem, ymin=mean-sem), position="dodge")+scale_x_discrete(breaks = 1:length(ethCondNames), labels=c("Attitude First", "TS First"))+
xlab("Order of Measures")+ylab("Mean Attitude Rating")+scale_fill_discrete(name="Ethnicity",labels=c("White","Immigrant"))+theme(legend.key=element_blank())
p3
##=========================== identification ratings =================================
#Cronbach's alpha, identification items
ident_mat_white <- array(0,dim=c(n_white,3))
ident_mat_white[,1] <- data$value.5[data$ETH_COND_NAME == "white"]
ident_mat_white[,2] <- data$like.5[data$ETH_COND_NAME == "white"]
ident_mat_white[,3] <- data$connected.5[data$ETH_COND_NAME == "white"]
cronbach(ident_mat_white) #0.93
ident_mat_immigrant <- array(0,dim=c(n_immigrant,3))
ident_mat_immigrant[,1] <- data$value.5[data$ETH_COND_NAME == "immigrant"]
ident_mat_immigrant[,2] <- data$like.5[data$ETH_COND_NAME == "immigrant"]
ident_mat_immigrant[,3] <- data$connected.5[data$ETH_COND_NAME == "immigrant"]
cronbach(ident_mat_immigrant) #0.89
# (3 questions averaged) How much do you agree: I like/value/feel connected to
# people from white/Arab backgrounds like Al/Alhadin
identification_aov <- aov(identification ~ vignette_num*eth*cond_num*order)
summary(identification_aov)
etaSquared(identification_aov)
#adjusted alphas
eth_aa_identification <- 0.05/15; eth_aa_identification
eth_order_aa_identification <- 0.05/14; eth_order_aa_identification
#lsmeans
means <- lsmeans(identification_aov, specs = ~ eth* order); means
tests <- contrast(means, method="pairwise"); tests
confint(tests)
tes(-5.167, n_white_1, n_immigrant_1) #cohen's d
tes(-0.434, n_white_2, n_immigrant_2) #cohen's d
## plot identification ratings (fig. 2)
p4<-ggplot(outgroup_mat.overall[,15:21],aes(x=factor(order),y=mean,fill=factor(eth)),color=factor(eth)) +
stat_summary(fun.y=mean,position=position_dodge(),geom="bar")+theme_bw()+coord_cartesian(ylim=c(1,9))+scale_y_discrete(limits=c("1","2","3","4","5","6","7","8","9"))
p4<- p4+theme(text = element_text(size=16),panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
geom_errorbar(aes(ymax=mean+sem, ymin=mean-sem), position="dodge")+scale_x_discrete(breaks = 1:length(ethCondNames), labels=c("Identification First", "TS First"))+
xlab("Order of Measures")+ylab("Mean Identification Rating")+scale_fill_discrete(name="Ethnicity",labels=c("White","Immigrant"))+theme(legend.key=element_blank())
p4
####======================================= end ========================================
threat <- as.numeric(data$THREAT.43)
attitude <- as.numeric(data$ATTITUDE.43)
identification <-as.numeric(data$IDENTIFICATION)
ts_scale <- as.numeric(data$TS_SCALE)
ts_forced <- as.factor(data$TS_FORCED_RECODE)
cond <- as.factor(data$COND_NAME)
cond_num <- as.factor(data$COND)
vignette <- as.factor(data$VIGNETTE_NAME)
vignette_num <- as.factor(data$VIGNETTE)
eth <- as.factor(data$ETH_COND_NAME)
eth_num <- as.factor(data$ETH_COND)
order_num <- as.factor(data$ORDER)
order <- vector(mode="character", length=dim(data)[1])
for (i in 1:dim(data)[1]) {
if(order_num[i] == 1) {
order[i] = 'intergroup first'
}
else if(order_num[i] == 2) {
order[i] = 'ts first'
}
}
gender <- as.factor(data$Gender)
age <- as.numeric(data$Age)
n_immigrant <- table(eth)[1]
n_white <- table(eth)[2]
## mean age and gender
mean(age,na.rm = TRUE) #37.14
table(gender)[2]/sum(table(gender)) #56.55%
##===== Aside: Export data for bayes analysis to be conducted in separate script ======
#dataExport <- data[,c(8,10,12,13,21,22)]
#dataExport <- as.data.frame(dataExport)
#write.csv(dataExport,"e2_tsIntergroup.csv")
##========================= true self forced choice =====================================
length(ts_forced[ts_forced == 1 & cond_num == 1])/length(ts_forced[cond_num == 1]) #proportion of improvement condition attributed to ts
length(ts_forced[ts_forced == 1 & cond_num == 2])/length(ts_forced[cond_num == 2]) #proportion of deterioration condition attributed to ts
ts_forced_glm_1 <- glm(ts_forced ~ cond_num, family = binomial)
summary(ts_forced_glm_1)
#see separate script for bayesian logistic regression in R
###===================== plot - true self forced choice (supp fig. 2) ===================
ethCondNames <- c("White American","Arab Immigrant")
accuracyTable <- matrix(NA,2,2)
rownames(accuracyTable) <- c('White American','Arab Immigrant')
colnames(accuracyTable) <- c('Improvement','Deterioration')
accuracyTable[1,1] <- sum(as.numeric(ts_forced[eth_num == 1 & cond_num == 1])-1)/length(ts_forced[eth_num == 1 & cond_num == 1])
accuracyTable[1,2] <- sum(as.numeric(ts_forced[eth_num == 1 & cond_num == 2])-1)/length(ts_forced[eth_num == 1 & cond_num == 2])
accuracyTable[2,1] <- sum(as.numeric(ts_forced[eth_num == 2 & cond_num == 1])-1)/length(ts_forced[eth_num == 2 & cond_num == 1])
accuracyTable[2,2] <- sum(as.numeric(ts_forced[eth_num == 2 & cond_num == 2])-1)/length(ts_forced[eth_num == 2 & cond_num == 2])
colors <- hue_pal()(3)
p = barplot(accuracyTable, beside=TRUE,ylim=c(0,1.0), col=colors[1:2],cex.names=1.5,cex.main=1.2,cex.main=1.7,
cex.lab=1.5,legend.text=FALSE,ylab="Proportion of Participants Attributing Change to True Self",main="True Self Forced Choice Items", family = "Helvetica")
legend("top", legend = ethCondNames, fill = colors[1:2], bty="n",cex = 1.3)
##========================= true self scaled rating =====================================
ts_scale_aov <- aov(ts_scale ~ cond_num*vignette_num*eth_num*order_num, data = data)
summary(ts_scale_aov)
etaSquared(ts_scale_aov)
#adjusted alphas
cond_aa_ts <- 0.05/15; cond_aa_ts
vignette_aa_ts <- 0.05/14; vignette_aa_ts
eth_aa_ts <- 0.05/13; eth_aa_ts
n_det <- table(cond)[1]
n_imp <- table(cond)[2]
#see separate JASP file for bayesian anova
ts_scale_mean <- tapply(ts_scale,cond,mean); ts_scale_mean
ts_scale_sd <- tapply(ts_scale,cond,sd); ts_scale_sd
ts_scaled_t <- t.test(ts_scale ~ cond, var.equal=TRUE, paired=FALSE); ts_scaled_t
tes(as.numeric(ts_scaled_t[1]), n_det, n_imp) #cohen's d
##====================== plot true self scaled rating ========================
alpha <- 0.05
condNames <- c("Improvement","Deterioration")
## condition effect by vignette
ts_vignettes <- matrix(NA,12,6)
colnames(ts_vignettes) <- c('cond','vignette','mean','sd','n','sem')
count <- 1
for(i in 1:2) {
for(k in 1:6) {
ts_vignettes[count,] <- c(i,k,mean(ts_scale[cond_num == i & vignette_num == k]),sd(ts_scale[cond_num == i & vignette_num == k]),length(ts_scale[cond_num == i & vignette_num == k]),0)
ts_vignettes[count,6] <- ts_vignettes[count,4]/sqrt(ts_vignettes[count,5]) #calculate standard error from mean
count = count+1;
}
}
ts_vignettes.summary <- as.data.frame(ts_vignettes, stringsAsFactors=F)
## effect of condition by vignette (supp fig. S3)
title <- c("Effect of Condition by Vignette")
p1<-ggplot(ts_vignettes.summary,aes(x=factor(cond),y=mean,fill=factor(vignette))) +
stat_summary(fun.y=mean,position=position_dodge(),geom="bar")+theme_bw()+coord_cartesian(ylim=c(1,9))+scale_y_discrete(limits=c("1","2","3","4","5","6","7","8","9"))
p1 <- p1+theme(text = element_text(size=18, family="Helvetica"),panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
geom_errorbar(aes(ymax=mean+sem, ymin=mean-sem), position="dodge")+ggtitle(title)+scale_x_discrete(breaks = 1:length(condNames), labels=condNames)+
xlab("Condition")+ylab("Mean True Self Attributions")+scale_fill_discrete(name="Vignette",labels=c("Caring/deadbeat father","Against/support terrorism","Ethical/unethical businessman","Teetotaler/alcoholic","Respect/mistreat minorities","Honest/corrupt officer"))+theme(legend.key=element_blank())+
theme(plot.title = element_text(hjust = 0.5))
p1
##=========================== threat ratings =================================
n_immigrant_1 = table(eth,order_num)[1,1]
n_white_1 = table(eth,order_num)[2,1]
n_immigrant_2 = table(eth,order_num)[1,2]
n_white_2 = table(eth,order_num)[2,2]
#How much do you agree: People from white/Arab backgrounds like Al/Alhadin threaten the
#american way of life
threat_aov <- aov(threat ~ vignette_num*eth*cond_num*order)
summary(threat_aov)
etaSquared(threat_aov)
#adjusted alphas
eth_by_order_aa_threat <- 0.05/15; eth_by_order_aa_threat
order_aa_threat <- 0.05/14; order_aa_threat
eth_aa_threat <- 0.05/13; eth_aa_threat
vignette_aa_threat <- 0.05/12; vignette_aa_threat
vignette_order_aa_threat <- 0.05/11; vignette_order_aa_threat
#lsmeans
means <- lsmeans(threat_aov, specs = ~ eth* order); means
tests <- contrast(means, method="pairwise"); tests
confint(tests)
tes(5.186, n_white_1, n_immigrant_1) #cohen's d
tes(-1.086, n_white_2, n_immigrant_2) #cohen's d
##=========================== plot threat ratings (fig. 2) ===================================
alpha <- 0.05
ethCondNames <- c("White American","Arab Immigrant")
out_mat_names <- c('threat','attitude','identification')
#OVERALL
outgroup_mat_overall <- array(0,dim=c(4,7,3)) #averaging across vignettes
colnames(outgroup_mat_overall) <- c('eth','order','mean','sd','n','sem','me')
count <- 1
for (h in 1:3) {
count = 1
for(k in 1:2) {
for(l in 1:2) {
outgroup_mat_overall[count,,h] <- c(k,l,mean(get(out_mat_names[h])[eth_num == k & order_num == l]),sd(get(out_mat_names[h])[eth_num == k & order_num == l]),length(get(out_mat_names[h])[eth_num == k & order_num == l]),0,0)
outgroup_mat_overall[count,6,h] <- outgroup_mat_overall[count,4,h]/sqrt(outgroup_mat_overall[count,5,h]) #calculate standard error from mean
outgroup_mat_overall[count,7,h] <- qt(1-alpha/2, df=outgroup_mat_overall[count,5,h]*outgroup_mat_overall[count,6,h]) #calculate margin of error
count = count+1
}
}
}
outgroup_mat.overall <- as.data.frame(outgroup_mat_overall, stringsAsFactors=F)
colnames(outgroup_mat.overall) <- c('eth','order','mean','sd','n','sem','me','eth','order','mean','sd','n','sem','me','eth','order','mean','sd','n','sem','me')
## overall means (fig. 2)
p2<-ggplot(outgroup_mat.overall[,1:7],aes(x=factor(order),y=mean,fill=factor(eth)),color=factor(eth)) +
stat_summary(fun.y=mean,position=position_dodge(),geom="bar")+theme_bw()+coord_cartesian(ylim=c(1,9))+scale_y_discrete(limits=c("1","2","3","4","5","6","7","8","9"))
p2<-p2+theme(text = element_text(size=16),panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
geom_errorbar(aes(ymax=mean+sem, ymin=mean-sem), position="dodge")+scale_x_discrete(breaks = 1:length(ethCondNames), labels=c("Threat First", "TS First"))+
xlab("Order of Measures")+ylab("Mean Threat Ratings")+scale_fill_discrete(name="Ethnicity",labels=c("White","Immigrant"))+theme(legend.key=element_blank())
p2
##=========================== attitude ratings =================================
#How do you feel about people from white/Arab backgrounds like Al/Alhadin? (1=cold,9=warm)
attitude_aov <- aov(attitude ~ vignette_num*eth*cond_num*order)
summary(attitude_aov)
etaSquared(attitude_aov)
#adjusted alphas
eth_order_aa_attitude <- 0.05/15; eth_order_aa_attitude
vignette_eth_cond_aa_attitude <- 0.05/14; vignette_eth_cond_aa_attitude
vignette_aa_attitude <- 0.05/13; vignette_aa_attitude
#lsmeans
means <- lsmeans(attitude_aov, specs = ~ eth* order); means
tests <- contrast(means, method="pairwise"); tests
confint(tests)
tes(3.124, n_white_1, n_immigrant_1) #cohen's d
tes(0.930, n_white_2, n_immigrant_2) #cohen's d
## plot attitude ratings (fig. 2)
p3<-ggplot(outgroup_mat.overall[,8:14],aes(x=factor(order),y=mean,fill=factor(eth)),color=factor(eth)) +
stat_summary(fun.y=mean,position=position_dodge(),geom="bar")+theme_bw()+coord_cartesian(ylim=c(1,9))+scale_y_discrete(limits=c("1","2","3","4","5","6","7","8","9"))
p3<- p3+theme(text = element_text(size=16),panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
geom_errorbar(aes(ymax=mean+sem, ymin=mean-sem), position="dodge")+scale_x_discrete(breaks = 1:length(ethCondNames), labels=c("Attitude First", "TS First"))+
xlab("Order of Measures")+ylab("Mean Attitude Rating")+scale_fill_discrete(name="Ethnicity",labels=c("White","Immigrant"))+theme(legend.key=element_blank())
p3
##=========================== identification ratings =================================
#Cronbach's alpha, identification items
ident_mat_white <- array(0,dim=c(n_white,3))
ident_mat_white[,1] <- data$value.5[data$ETH_COND_NAME == "white"]
ident_mat_white[,2] <- data$like.5[data$ETH_COND_NAME == "white"]
ident_mat_white[,3] <- data$connected.5[data$ETH_COND_NAME == "white"]
cronbach(ident_mat_white) #0.93
ident_mat_immigrant <- array(0,dim=c(n_immigrant,3))
ident_mat_immigrant[,1] <- data$value.5[data$ETH_COND_NAME == "immigrant"]
ident_mat_immigrant[,2] <- data$like.5[data$ETH_COND_NAME == "immigrant"]
ident_mat_immigrant[,3] <- data$connected.5[data$ETH_COND_NAME == "immigrant"]
cronbach(ident_mat_immigrant) #0.89
# (3 questions averaged) How much do you agree: I like/value/feel connected to
# people from white/Arab backgrounds like Al/Alhadin
identification_aov <- aov(identification ~ vignette_num*eth*cond_num*order)
summary(identification_aov)
etaSquared(identification_aov)
#adjusted alphas
eth_aa_identification <- 0.05/15; eth_aa_identification
eth_order_aa_identification <- 0.05/14; eth_order_aa_identification
#lsmeans
means <- lsmeans(identification_aov, specs = ~ eth* order); means
tests <- contrast(means, method="pairwise"); tests
confint(tests)
tes(-5.167, n_white_1, n_immigrant_1) #cohen's d
tes(-0.434, n_white_2, n_immigrant_2) #cohen's d
## plot identification ratings (fig. 2)
p4<-ggplot(outgroup_mat.overall[,15:21],aes(x=factor(order),y=mean,fill=factor(eth)),color=factor(eth)) +
stat_summary(fun.y=mean,position=position_dodge(),geom="bar")+theme_bw()+coord_cartesian(ylim=c(1,9))+scale_y_discrete(limits=c("1","2","3","4","5","6","7","8","9"))
p4<- p4+theme(text = element_text(size=16),panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
geom_errorbar(aes(ymax=mean+sem, ymin=mean-sem), position="dodge")+scale_x_discrete(breaks = 1:length(ethCondNames), labels=c("Identification First", "TS First"))+
xlab("Order of Measures")+ylab("Mean Identification Rating")+scale_fill_discrete(name="Ethnicity",labels=c("White","Immigrant"))+theme(legend.key=element_blank())
p4
####======================================= end ========================================
## 6 July 2016
## Author: Julian De Freitas, Harvard University
## clear workpace
rm(list = ls())
## necessary packages
if (!require(ggplot2)) {install.packages("ggplot2"); require(ggplot2)}
if (!require(plyr)) {install.packages("plyr"); require(plyr)}
if (!require(lsr)) {install.packages("lsr"); require(lsr)}
if (!require(psy)) {install.packages("psy"); require(psy)}
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
if (!require(broom)) {install.packages("broom"); require(broom)}
if (!require(bbmle)) {install.packages("bbmle"); require(bbmle)}
## get data
dir <- setwd("/Users/Julian/Dropbox (Personal)/Research/socialPsych/intergroupTS/intergroupTS_onlineMaterials/e3_eliminate_behavior/data_and_analyses")
data <- read.csv("e3_eliminate_behavior.csv")
head(data)
dim(data) # 678
## remove unnecessary columns
data <- data[,c(4,5,154:172)]
## exclusions: only incl. white subjects who answered attention and comprehension checks correctly
data <- subset(data,(data$exclude == 0))
dim(data) #374
## Recode donation values that were coded weirdly (but consistently) in Qualtrics. Code from 1 = ARC to 11 - SARC
donationRecode_mat <- c(12,13,23,14,15,16,18,19,9,20,21)
for (i in 1:dim(data)[1]) {
data$donation[i] <- match(data$donation[i],donationRecode_mat)
}
#add ts_forced_recode
for (i in 1:dim(data)[1]) {
if(data$ts_forced[i] == 1) {
data$ts_forced_recode[i] = 1
}
else if(data$ts_forced[i] == 2 | data$ts_forced[i] == 3) {
data$ts_forced_recode[i] = 0
}
}
## Assign variable names
ts_time <- as.factor(data$ts_time)
ts_forced <- as.factor(data$ts_forced_recode)
ts_scaled <- as.numeric(data$ts_scale)
donation_time <- as.numeric(data$donation_time)
donation <- as.numeric(data$donation)
vignette <- as.factor(data$vignette_name)
vignette_num <- as.factor(data$vignette)
eth <- as.factor(data$eth)
eth_name <- as.factor(data$eth_name)
order <- as.factor(data$order)
order_name <- as.factor(data$order_name)
gender <- as.factor(data$Gender)
age <- as.numeric(data$Age)
table(eth,order)
table(eth,order,vignette)
## Mean age and gender
mean(age,na.rm = TRUE) #36.71
table(gender)[2]/sum(table(gender)) #52.01%
##===== Aside: Export data for bayes analysis to be conducted in separate script ======
#dataExport <- data[,c(5,7,9,12,22)]
#dataExport <- as.data.frame(dataExport)
#write.csv(dataExport,"e3_tsIntergroup.csv")
##========================= true self forced choice =====================================
length(ts_forced[ts_forced == 1 & eth == 1])/length(ts_forced[eth == 1]) #proportion of improvement condition attributed to ts
length(ts_forced[ts_forced == 1 & eth == 2])/length(ts_forced[eth == 2]) #proportion of deterioration condition attributed to ts
ts_forced_glm_1 <- glm(ts_forced ~ eth, family = 'binomial')
summary(ts_forced_glm_1)
#see separate script for bayesian logistic regression in R
##========================= true self scaled =====================================
ts_s_model <- aov(ts_scale ~ eth*order*vignette_num, data = data) #ts scaled
summary(ts_s_model)
etaSquared(ts_s_model)
#see separate JASP file for bayesian anova
##===================================== donations =======================================
alpha <- 0.05
ethNames <- c("White","Immigrant")
orderNames <- c("Ts First", "Donation First")
n_order1 <- table(order)[1] #ts first
n_order2 <- table(order)[2] #donation first
donation_model <- aov(donation ~ eth*order*vignette_num, data = data)
summary(donation_model)
etaSquared(donation_model)
#adjusted alphas
order_aa_donation <- 0.05/7; order_aa_donation
donation_mean <- tapply(donation,order,mean); donation_mean
donation_sd <- tapply(donation,order,sd); donation_sd
donation_t <- t.test(donation ~ order, var.equal=TRUE, paired=FALSE); donation_t
tes(as.numeric(donation_t[1]), n_order1, n_order2) #cohen's d
##================================== plot donations (fig. 3) ======================================
donation_mat_detailed <- array(0,dim=c(4,7))
colnames(donation_mat_detailed) <- c('row','order','eth','mean','sd','n','sem')
donation_mat_detailed[1,] <- c(1,2,1,mean(donation[order == 2 & eth == 1]),sd(donation[order == 2 & eth == 1]),length(donation[order == 2 & eth == 1]),0)
donation_mat_detailed[2,] <- c(1,2,2,mean(donation[order == 2 & eth == 2] ),sd(donation[order == 2 & eth == 2]),length(donation[order == 2 & eth == 2]),0)
donation_mat_detailed[3,] <- c(2,1,1,mean(donation[order == 1 & eth == 1]),sd(donation[order == 1 & eth == 1]),length(donation[order == 1 & eth == 1]),0)
donation_mat_detailed[4,] <- c(2,1,2,mean(donation[order == 1 & eth == 2]),sd(donation[order == 1 & eth == 2]),length(donation[order == 1 & eth == 2]),0)
for (i in 1:4) {
donation_mat_detailed[i,7] <- donation_mat_detailed[i,5]/sqrt(donation_mat_detailed[i,6])
}
donationDetailed.mat <- as.data.frame(donation_mat_detailed, stringsAsFactors=F)
title <- c("Donations")
p1<-ggplot(donationDetailed.mat,aes(x=factor(row),y=mean,fill=factor(eth))) +
stat_summary(fun.y=mean,position=position_dodge(),geom="bar")+theme_bw()+coord_cartesian(ylim=c(1,11))+scale_y_discrete(limits=c("0","10","20","30","40","50","60","70","80","90","100"))
p1 <- p1+theme(text = element_text(size=16),panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
geom_errorbar(aes(ymax=mean+sem, ymin=mean-sem), position="dodge")+scale_x_discrete(breaks = 1:2, labels=c("Donation First", "TS First"))+
xlab("Order of Measures")+ylab("Mean Donation (%) to Outgroup Charity")+scale_fill_discrete(name="Ethnicity",labels=c("White","Immigrant"))+theme(legend.key=element_blank())
p1
####======================================= end ========================================
rm(list = ls())
## necessary packages
if (!require(ggplot2)) {install.packages("ggplot2"); require(ggplot2)}
if (!require(plyr)) {install.packages("plyr"); require(plyr)}
if (!require(lsr)) {install.packages("lsr"); require(lsr)}
if (!require(psy)) {install.packages("psy"); require(psy)}
if (!require(compute.es)) {install.packages("compute.es"); require(compute.es)}
if (!require(broom)) {install.packages("broom"); require(broom)}
if (!require(bbmle)) {install.packages("bbmle"); require(bbmle)}
## get data
dir <- setwd("/Users/Julian/Dropbox (Personal)/Research/socialPsych/intergroupTS/intergroupTS_onlineMaterials/e3_eliminate_behavior/data_and_analyses")
data <- read.csv("e3_eliminate_behavior.csv")
head(data)
dim(data) # 678
## remove unnecessary columns
data <- data[,c(4,5,154:172)]
## exclusions: only incl. white subjects who answered attention and comprehension checks correctly
data <- subset(data,(data$exclude == 0))
dim(data) #374
## Recode donation values that were coded weirdly (but consistently) in Qualtrics. Code from 1 = ARC to 11 - SARC
donationRecode_mat <- c(12,13,23,14,15,16,18,19,9,20,21)
for (i in 1:dim(data)[1]) {
data$donation[i] <- match(data$donation[i],donationRecode_mat)
}
#add ts_forced_recode
for (i in 1:dim(data)[1]) {
if(data$ts_forced[i] == 1) {
data$ts_forced_recode[i] = 1
}
else if(data$ts_forced[i] == 2 | data$ts_forced[i] == 3) {
data$ts_forced_recode[i] = 0
}
}
## Assign variable names
ts_time <- as.factor(data$ts_time)
ts_forced <- as.factor(data$ts_forced_recode)
ts_scaled <- as.numeric(data$ts_scale)
donation_time <- as.numeric(data$donation_time)
donation <- as.numeric(data$donation)
vignette <- as.factor(data$vignette_name)
vignette_num <- as.factor(data$vignette)
eth <- as.factor(data$eth)
eth_name <- as.factor(data$eth_name)
order <- as.factor(data$order)
order_name <- as.factor(data$order_name)
gender <- as.factor(data$Gender)
age <- as.numeric(data$Age)
table(eth,order)
table(eth,order,vignette)
## Mean age and gender
mean(age,na.rm = TRUE) #36.71
table(gender)[2]/sum(table(gender)) #52.01%
##===== Aside: Export data for bayes analysis to be conducted in separate script ======
#dataExport <- data[,c(5,7,9,12,22)]
#dataExport <- as.data.frame(dataExport)
#write.csv(dataExport,"e3_tsIntergroup.csv")
##========================= true self forced choice =====================================
length(ts_forced[ts_forced == 1 & eth == 1])/length(ts_forced[eth == 1]) #proportion of improvement condition attributed to ts
length(ts_forced[ts_forced == 1 & eth == 2])/length(ts_forced[eth == 2]) #proportion of deterioration condition attributed to ts
ts_forced_glm_1 <- glm(ts_forced ~ eth, family = 'binomial')
summary(ts_forced_glm_1)
#see separate script for bayesian logistic regression in R
##========================= true self scaled =====================================
ts_s_model <- aov(ts_scale ~ eth*order*vignette_num, data = data) #ts scaled
summary(ts_s_model)
etaSquared(ts_s_model)
#see separate JASP file for bayesian anova
##===================================== donations =======================================
alpha <- 0.05
ethNames <- c("White","Immigrant")
orderNames <- c("Ts First", "Donation First")
n_order1 <- table(order)[1] #ts first
n_order2 <- table(order)[2] #donation first
donation_model <- aov(donation ~ eth*order*vignette_num, data = data)
summary(donation_model)
etaSquared(donation_model)
#adjusted alphas
order_aa_donation <- 0.05/7; order_aa_donation
donation_mean <- tapply(donation,order,mean); donation_mean
donation_sd <- tapply(donation,order,sd); donation_sd
donation_t <- t.test(donation ~ order, var.equal=TRUE, paired=FALSE); donation_t
tes(as.numeric(donation_t[1]), n_order1, n_order2) #cohen's d
##================================== plot donations (fig. 3) ======================================
donation_mat_detailed <- array(0,dim=c(4,7))
colnames(donation_mat_detailed) <- c('row','order','eth','mean','sd','n','sem')
donation_mat_detailed[1,] <- c(1,2,1,mean(donation[order == 2 & eth == 1]),sd(donation[order == 2 & eth == 1]),length(donation[order == 2 & eth == 1]),0)
donation_mat_detailed[2,] <- c(1,2,2,mean(donation[order == 2 & eth == 2] ),sd(donation[order == 2 & eth == 2]),length(donation[order == 2 & eth == 2]),0)
donation_mat_detailed[3,] <- c(2,1,1,mean(donation[order == 1 & eth == 1]),sd(donation[order == 1 & eth == 1]),length(donation[order == 1 & eth == 1]),0)
donation_mat_detailed[4,] <- c(2,1,2,mean(donation[order == 1 & eth == 2]),sd(donation[order == 1 & eth == 2]),length(donation[order == 1 & eth == 2]),0)
for (i in 1:4) {
donation_mat_detailed[i,7] <- donation_mat_detailed[i,5]/sqrt(donation_mat_detailed[i,6])
}
donationDetailed.mat <- as.data.frame(donation_mat_detailed, stringsAsFactors=F)
title <- c("Donations")
p1<-ggplot(donationDetailed.mat,aes(x=factor(row),y=mean,fill=factor(eth))) +
stat_summary(fun.y=mean,position=position_dodge(),geom="bar")+theme_bw()+coord_cartesian(ylim=c(1,11))+scale_y_discrete(limits=c("0","10","20","30","40","50","60","70","80","90","100"))
p1 <- p1+theme(text = element_text(size=16),panel.grid.major = element_blank(),panel.grid.minor = element_blank())+
geom_errorbar(aes(ymax=mean+sem, ymin=mean-sem), position="dodge")+scale_x_discrete(breaks = 1:2, labels=c("Donation First", "TS First"))+
xlab("Order of Measures")+ylab("Mean Donation (%) to Outgroup Charity")+scale_fill_discrete(name="Ethnicity",labels=c("White","Immigrant"))+theme(legend.key=element_blank())
p1
####======================================= end ========================================
